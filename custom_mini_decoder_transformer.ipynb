{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1607621",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This code is suggested by Tabnine using the following prompt:\n",
    "Prompt: \"I need to build a custom transformer model to predict an output sequence from an input one. Provide a concise Pytorch codebase to train a decoder transformer model on a custom dataset. Don't use a pre-trained model, we need to build all the transformer layers from scratch using Pytorch API.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304b04ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016f11c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the custom tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_dict = {i: chr(97 + i) for i in range(vocab_size)}\n",
    "        self.inv_token_dict = {v: k for k, v in self.token_dict.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.token_dict[c] for c in text]\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        return ''.join([self.inv_token_dict[i] for i in encoded_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2402f8b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.data[idx][0]\n",
    "        target_text = self.data[idx][1]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text)\n",
    "        target_ids = self.tokenizer.encode(target_text)\n",
    "\n",
    "        if len(input_ids) < self.max_len:\n",
    "            input_ids = input_ids + [0] * (self.max_len - len(input_ids))\n",
    "\n",
    "        if len(target_ids) < self.max_len:\n",
    "            target_ids = target_ids + [0] * (self.max_len - len(target_ids))\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'target_ids': torch.tensor(target_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a224083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c435f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the custom transformer model\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, n_heads, dropout_rate):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, max_len)\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_dim, n_heads, dropout_rate)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.linear_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        input_embeds = self.embedding(input_ids)\n",
    "        input_embeds = self.pos_encoding(input_embeds)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            input_embeds = layer(input_embeds, attention_mask)\n",
    "\n",
    "        output = self.linear_out(input_embeds)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d5211",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243dba5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the transformer layer\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attention = MultiheadAttention(d_model, n_heads, dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask):\n",
    "        attn_output, _ = self.multihead_attention(input_embeds, input_embeds, input_embeds, attention_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        out1 = self.norm1(attn_output + input_embeds)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        out2 = self.norm2(ffn_output + out1)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136e8ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the multihead attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout_rate):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, attention_mask):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        query = self.q_proj(query)\n",
    "        key = self.k_proj(key)\n",
    "        value = self.v_proj(value)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.n_heads, -1)\n",
    "        key = key.view(batch_size, seq_len, self.n_heads, -1)\n",
    "        value = value.view(batch_size, seq_len, self.n_heads, -1)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_model // self.n_heads)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        output = output.view(batch_size, seq_len, self.n_heads * self.d_model)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output, attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174d4f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the feedforward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout_rate):\n",
    "        super(FeedForward, __init__).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 2)\n",
    "        self.linear2 = nn.Linear(d_model * 2, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(-1))\n",
    "        x = self.linear2(torch.relu(self.linear1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x.view(x.size(0), x.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b1d67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, optimizer, data_loader, device, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_ids)\n",
    "            loss = nn.CrossEntropyLoss()(output, target_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948d5c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main():\n",
    "    # Define the hyperparameters\n",
    "    input_dim = 100\n",
    "    hidden_dim = 512\n",
    "    output_dim = 100\n",
    "    n_layers = 6\n",
    "    n_heads = 8\n",
    "    dropout_rate = 0.1\n",
    "    max_len = 50\n",
    "    batch_size = 32\n",
    "    lr = 0.001\n",
    "    epochs = 10\n",
    "\n",
    "    # Define the tokenizer\n",
    "    tokenizer = Tokenizer(input_dim)\n",
    "\n",
    "    # Define the custom dataset\n",
    "    data = [\n",
    "        ('input_text_1', 'target_text_1'),\n",
    "        ('input_text_2', 'target_text_2'),\n",
    "        # Add more data here\n",
    "    ]\n",
    "    dataset = CustomDataset(data, tokenizer, max_len)\n",
    "\n",
    "    # Define the data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Define the transformer model\n",
    "    model = CustomTransformer(input_dim, hidden_dim, output_dim, n_layers, n_heads, dropout_rate).to(device)\n",
    "\n",
    "    print(\"The model info:\", model)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model\n",
    "    # train(model, optimizer, data_loader, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".ccp_simulator_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
