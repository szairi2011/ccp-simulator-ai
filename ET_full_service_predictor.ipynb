{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sample dataset from templates\n",
    "\n",
    "1. CCP TO SGW : \n",
    "\n",
    "<?xml version=\"1.0\" ?><FIXML xmlns:ns2=\"http://www.fixprotocol.org/FIXML-Latest/METADATA\"><TrdCaptRpt RptID=\"2799\" TrdID=\"30281004800\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"2\" MtchID=\"30281001777\" PxTyp=\"2\" LastQty=\"10\" LastPx=\"115\" TrdDt=\"2023-09-28\" BizDt=\"2023-09-28\" TxnTm=\"2023-10-04T05:17:30.749-05:00\"><Hdr SID=\"MGEX\" TID=\"D210\" PosDup=\"N\" PosRsnd=\"N\" Snt=\"2023-10-04T05:17:30.788-05:00\"></Hdr><Instrmt Sym=\"BUI\" ID=\"BUI\" Src=\"H\" CFI=\"FCCPXX\" MMY=\"202312\" MatDt=\"2023-12-27\" Mult=\"0.1\" Exch=\"BTNL\"></Instrmt><RptSide Side=\"2\" Ccy=\"USD\" InptSrc=\"MA\" InptDev=\"PORTAL\" CustCpcty=\"1\" PosEfct=\"C\" ClOrdID=\"123\" MLegRptTyp=\"1\" AllocInd=\"0\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"210\" R=\"4\"></Pty><Pty ID=\"Emira-115\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"210\" R=\"12\"></Pty><TrdRegTS TS=\"2023-10-04T05:17:30.749-05:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt></FIXML>\n",
    "\n",
    "\n",
    "2. SGW TO CCP : \n",
    "\n",
    "<FIXML xmlns=\"http://www.fixprotocol.org/FIXML-Latest\"><TrdCaptRpt RptID=\"30281004800\" TrdDt=\"2023-09-28\" BizDt=\"2023-09-28\" TxnTm=\"2023-10-04T05:17:30.749-05:00\" TrdID=\"30281004800\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"10.0\" LastPx=\"115.0\"><Hdr Snt=\"2023-10-04T11:18:32.348+01:00\" TID=\"MGEX\" SID=\"D210\"/><Instrmt Exch=\"BTNL\" ID=\"BUI\" MMY=\"202312\" CFI=\"FXXXXX\"/><RptSide Side=\"2\" CustCpcty=\"2\"><Pty ID=\"210\" R=\"1\"/><Pty ID=\"FIRMACT1\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt></FIXML>\n",
    "\n",
    "\n",
    "3. CCP TO SGW : \n",
    "\n",
    "<?xml version=\"1.0\" ?><FIXML xmlns:ns2=\"http://www.fixprotocol.org/FIXML-Latest/METADATA\"><TrdCaptRpt RptID=\"2801\" TrdID=\"30281004800\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"2\" MtchID=\"30281001777\" PxTyp=\"2\" LastQty=\"10\" LastPx=\"115\" TrdDt=\"2023-09-28\" BizDt=\"2023-09-28\" TxnTm=\"2023-10-04T05:18:33.138-05:00\"><Hdr SID=\"MGEX\" TID=\"D210\" PosDup=\"N\" PosRsnd=\"N\" Snt=\"2023-10-04T05:18:33.160-05:00\"></Hdr><Instrmt Sym=\"BUI\" ID=\"BUI\" Src=\"H\" CFI=\"FCCPXX\" MMY=\"202312\" MatDt=\"2023-12-27\" Mult=\"0.1\" Exch=\"BTNL\"></Instrmt><RptSide Side=\"2\" Ccy=\"USD\" InptSrc=\"MA\" InptDev=\"PORTAL\" CustCpcty=\"2\" PosEfct=\"C\" ClOrdID=\"123\" MLegRptTyp=\"1\" AllocInd=\"0\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"210\" R=\"4\"></Pty><Pty ID=\"FIRMACT1\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"210\" R=\"12\"></Pty><TrdRegTS TS=\"2023-10-04T05:17:30.749-05:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt></FIXML>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_service_wf():\n",
    "  rptid = random.randint(1000,400000)\n",
    "  trdid = random.randint(20281004800, 40281004800)\n",
    "  trdtyp = random.choice([0, 1, 2, 3, 11, 15, 20, 29, 31, 45])\n",
    "  mtchid = random.randint(20281001777, 40281001777)\n",
    "  pxtyp = random.choice([2, 10])\n",
    "  lastqty = random.randint(10,100)\n",
    "  lastpx = random.choice([100, 370, 876, 250.45, 300.9, 540.698, 1050, 3590.59])\n",
    "  trdt = random.choice([\"2022-12-17\",\"2022-12-21\",\"2023-01-02\",\"2023-01-05\"])\n",
    "  bizdt = random.choice([\"2023-01-06\",\"2023-01-11\",\"2023-01-20\",\"2023-02-05\",\"2023-02-10\"])\n",
    "  t = random.choice([\"2023-02-15\",\"2023-02-21\",\"2023-03-20\",\"2023-03-30\"])\n",
    "  time1 = random.choice(['T05:17:30','T10:18:15','T12:25:13','T16:40:55'])\n",
    "  r1 = random.choice(['.100','.230','.326'])\n",
    "  r2 = random.choice(['.350','.470','.768'])\n",
    "  tz1 = random.choice(['-5:00','-6:00','-7:00'])\n",
    "  txntm= t+time1+r1+tz1\n",
    "  time2 = random.choice(['T17:30:55.600','T19:33:05.333','T22:03:12.436'])\n",
    "  snt1 = t+time1+r2+tz1\n",
    "  snt2 = t+time2+'+01:00'\n",
    "  snt3 = t+'T00:12:55.138'+tz1\n",
    "  snt4 = t+'T00:12:55.160'+tz1\n",
    "  sym = random.choice([\"BUI\",\"ABP\",\"TMP\"])\n",
    "  id5 = random.choice(['B','W','AS','LM','BUI','IP0'])\n",
    "  my = random.choice([\"2023-05\",\"2023-07\",\"2023-10\"])\n",
    "  mmy = my.replace('-','')\n",
    "  matdt = my+'-'+str(random.randint(1,28))\n",
    "  inptsrc = random.choice(['MA','EL','SY'])\n",
    "  clordid = random.choice(['123','134','145','146ZBT','245BTN','987NL','OrdAT'])\n",
    "  tid = random.choice(['ABCD','D210','B509','HBPL','TL98'])\n",
    "  id2 = random.choice(['Emira-115','Ahlem-110','Karim-200','Mohamed-300','acc-AT','bl-NL','Mariem','ahmed'])\n",
    "  typ = random.randint(20,30)\n",
    "  id3 = random.choice([\"FIRMACT1\",\"AKLPM9\",\"MPLAMP6\",\"00120007\",\"Hassen\",\"Ahlem\",'arij'])\n",
    "  custcpcty1 = random.randint(1,4)\n",
    "  custcpcty2 = random.randint(1,4)\n",
    "  ex = random.choice(['BTNL','SNTL','HDPL','XMGE'])\n",
    "  mult = random.choice([0.1, 0.2, 0.3, 0.4, 0.9, 1000, 2000, 5000, 300, 200, 50, 70])\n",
    "  pos1 = random.choice(['Y','N'])\n",
    "  pos3 = random.choice(['Y','N'])\n",
    "  cfi = random.choice(['FCCPXX','APYBKL','MKLPON','FCAPSX'])\n",
    "  pos2 = random.choice(['O','C'])\n",
    "  side = random.randint(1,2)\n",
    "  id26 = random.randint(1,2)\n",
    "  mlrty = random.randint(1,3)\n",
    "  alin = random.randint(0,1)\n",
    "  inpdev = random.choice(['PORTAL','EXCHANGE','API','CLEARING'])\n",
    "  id4 = random.randint(100,500)\n",
    "  id12 = random.choice(['100','200','400','A123','B115'])\n",
    "  cfi2 = random.choice(['FCCPXX','APYBKL','MKLPON','FCAPSX'])\n",
    "  return (\n",
    "      f\"\"\"<TrdCaptRpt RptID=\"{rptid}\" TrdID=\"{trdid}\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt1}\"></Hdr>\n",
    "<Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty1}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n",
    "<Pty ID=\"{id2}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\">\n",
    "</TrdRegTS></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n",
    "      \n",
    "      f\"\"\"<TrdCaptRpt RptID=\"{trdid}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"{float(lastqty)}\" LastPx=\"{float(lastpx)}\"><Hdr Snt=\"{snt2}\" TID=\"MGEX\" SID=\"{tid}\"/><Instrmt Exch=\"{ex}\" ID=\"{id5}\" MMY=\"{mmy}\" CFI=\"{cfi2}\"/><RptSide Side=\"{side}\" CustCpcty=\"{custcpcty2}\">\n",
    "<Pty ID=\"210\" R=\"1\"/><Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n",
    "\n",
    "      f\"\"\"<TrdCaptRpt RptID=\"580198\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{snt3}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt4}\"></Hdr>\n",
    "<Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty2}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n",
    "<Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\"></TrdRegTS></RptSide>\n",
    "</TrdCaptRpt>\"\"\".replace('\\n', '')\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<TrdCaptRpt RptID=\"340622\" TrdID=\"30043555544\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"29\" MtchID=\"39431742936\" PxTyp=\"2\" LastQty=\"24\" LastPx=\"540.698\" TrdDt=\"2022-12-21\" BizDt=\"2023-01-20\" TxnTm=\"2023-02-21T16:40:55.230-6:00\"><Hdr SID=\"MGEX\" TID=\"B509\" PosDup=\"Y\" PosRsnd=\"Y\" Snt=\"2023-02-21T16:40:55.350-6:00\"></Hdr><Instrmt Sym=\"ABP\" ID=\"W\" Src=\"H\" CFI=\"MKLPON\" MMY=\"202310\" MatDt=\"2023-10-8\" Mult=\"5000\" Exch=\"XMGE\"></Instrmt><RptSide Side=\"2\" Ccy=\"USD\" InptSrc=\"SY\" InptDev=\"CLEARING\" CustCpcty=\"3\" PosEfct=\"O\" ClOrdID=\"146ZBT\" MLegRptTyp=\"1\" AllocInd=\"1\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"186\" R=\"4\"></Pty><Pty ID=\"Ahlem-110\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"400\" R=\"12\"></Pty><TrdRegTS TS=\"2023-02-21T16:40:55.230-6:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt>', '<TrdCaptRpt RptID=\"30043555544\" TrdDt=\"2022-12-21\" BizDt=\"2023-01-20\" TxnTm=\"2023-02-21T16:40:55.230-6:00\" TrdID=\"30043555544\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"24.0\" LastPx=\"540.698\"><Hdr Snt=\"2023-02-21T19:33:05.333+01:00\" TID=\"MGEX\" SID=\"B509\"/><Instrmt Exch=\"XMGE\" ID=\"W\" MMY=\"202310\" CFI=\"MKLPON\"/><RptSide Side=\"2\" CustCpcty=\"1\"><Pty ID=\"210\" R=\"1\"/><Pty ID=\"Ahlem\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>', '<TrdCaptRpt RptID=\"580198\" TrdID=\"30043555544\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"29\" MtchID=\"39431742936\" PxTyp=\"2\" LastQty=\"24\" LastPx=\"540.698\" TrdDt=\"2022-12-21\" BizDt=\"2023-01-20\" TxnTm=\"2023-02-21T00:12:55.138-6:00\"><Hdr SID=\"MGEX\" TID=\"B509\" PosDup=\"Y\" PosRsnd=\"Y\" Snt=\"2023-02-21T00:12:55.160-6:00\"></Hdr><Instrmt Sym=\"ABP\" ID=\"W\" Src=\"H\" CFI=\"MKLPON\" MMY=\"202310\" MatDt=\"2023-10-8\" Mult=\"5000\" Exch=\"XMGE\"></Instrmt><RptSide Side=\"2\" Ccy=\"USD\" InptSrc=\"SY\" InptDev=\"CLEARING\" CustCpcty=\"1\" PosEfct=\"O\" ClOrdID=\"146ZBT\" MLegRptTyp=\"1\" AllocInd=\"1\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"186\" R=\"4\"></Pty><Pty ID=\"Ahlem\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"400\" R=\"12\"></Pty><TrdRegTS TS=\"2023-02-21T16:40:55.230-6:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt>')\n"
     ]
    }
   ],
   "source": [
    "xml_sample = generate_full_service_wf()\n",
    "# xml_sample = generate_fs_samples(**patterns)\n",
    "print(xml_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [generate_full_service_wf() for _ in range(1000)]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"29\" MtchID=\"37713690660\" PxTyp=\"10\" LastQty=\"12\" LastPx=\"250.45\" TrdDt=\"2022-12-21\" BizDt=\"2023-02-10\" TxnTm=\"2023-02-15T12:25:13.230-5:00\"><Hdr SID=\"MGEX\" TID=\"D210\" PosDup=\"N\" PosRsnd=\"Y\" Snt=\"2023-02-15T12:25:13.768-5:00\"></Hdr><Instrmt Sym=\"ABP\" ID=\"AS\" Src=\"H\" CFI=\"APYBKL\" MMY=\"202310\" MatDt=\"2023-10-2\" Mult=\"50\" Exch=\"SNTL\"></Instrmt><RptSide Side=\"1\" Ccy=\"USD\" InptSrc=\"SY\" InptDev=\"API\" CustCpcty=\"1\" PosEfct=\"C\" ClOrdID=\"OrdAT\" MLegRptTyp=\"1\" AllocInd=\"0\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"500\" R=\"4\"></Pty><Pty ID=\"ahmed\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"A123\" R=\"12\"></Pty><TrdRegTS TS=\"2023-02-15T12:25:13.230-5:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TrdCaptRpt RptID=\"31270713000\" TrdDt=\"2022-12-21\" BizDt=\"2023-02-10\" TxnTm=\"2023-02-15T12:25:13.230-5:00\" TrdID=\"31270713000\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"12.0\" LastPx=\"250.45\"><Hdr Snt=\"2023-02-15T17:30:55.600+01:00\" TID=\"MGEX\" SID=\"D210\"/><Instrmt Exch=\"SNTL\" ID=\"AS\" MMY=\"202310\" CFI=\"MKLPON\"/><RptSide Side=\"1\" CustCpcty=\"2\"><Pty ID=\"210\" R=\"1\"/><Pty ID=\"MPLAMP6\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"29\" MtchID=\"37713690660\" PxTyp=\"10\" LastQty=\"12\" LastPx=\"250.45\" TrdDt=\"2022-12-21\" BizDt=\"2023-02-10\" TxnTm=\"2023-02-15T00:12:55.138-5:00\"><Hdr SID=\"MGEX\" TID=\"D210\" PosDup=\"N\" PosRsnd=\"Y\" Snt=\"2023-02-15T00:12:55.160-5:00\"></Hdr><Instrmt Sym=\"ABP\" ID=\"AS\" Src=\"H\" CFI=\"APYBKL\" MMY=\"202310\" MatDt=\"2023-10-2\" Mult=\"50\" Exch=\"SNTL\"></Instrmt><RptSide Side=\"1\" Ccy=\"USD\" InptSrc=\"SY\" InptDev=\"API\" CustCpcty=\"2\" PosEfct=\"C\" ClOrdID=\"OrdAT\" MLegRptTyp=\"1\" AllocInd=\"0\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"500\" R=\"4\"></Pty><Pty ID=\"MPLAMP6\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"A123\" R=\"12\"></Pty><TrdRegTS TS=\"2023-02-15T12:25:13.230-5:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Executed Trade</th>\n",
       "      <th>Full Service Operation</th>\n",
       "      <th>CCP Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" ...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"31270713000\" TrdDt=\"2022-12...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"131755\" TrdID=\"36410189235\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"36410189235\" TrdDt=\"2022-12...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"42663\" TrdID=\"28319621580\" ...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"28319621580\" TrdDt=\"2023-01...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"317405\" TrdID=\"23327228661\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"23327228661\" TrdDt=\"2023-01...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"86846\" TrdID=\"39125469483\" ...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"39125469483\" TrdDt=\"2022-12...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Executed Trade  \\\n",
       "0  <TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" ...   \n",
       "1  <TrdCaptRpt RptID=\"131755\" TrdID=\"36410189235\"...   \n",
       "2  <TrdCaptRpt RptID=\"42663\" TrdID=\"28319621580\" ...   \n",
       "3  <TrdCaptRpt RptID=\"317405\" TrdID=\"23327228661\"...   \n",
       "4  <TrdCaptRpt RptID=\"86846\" TrdID=\"39125469483\" ...   \n",
       "\n",
       "                              Full Service Operation  \\\n",
       "0  <TrdCaptRpt RptID=\"31270713000\" TrdDt=\"2022-12...   \n",
       "1  <TrdCaptRpt RptID=\"36410189235\" TrdDt=\"2022-12...   \n",
       "2  <TrdCaptRpt RptID=\"28319621580\" TrdDt=\"2023-01...   \n",
       "3  <TrdCaptRpt RptID=\"23327228661\" TrdDt=\"2023-01...   \n",
       "4  <TrdCaptRpt RptID=\"39125469483\" TrdDt=\"2022-12...   \n",
       "\n",
       "                                        CCP Response  \n",
       "0  <TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...  \n",
       "1  <TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...  \n",
       "2  <TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...  \n",
       "3  <TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...  \n",
       "4  <TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...  "
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Dataframe from the generated samples\n",
    "ccp_new_trade_messages = [message[0] for message in dataset]\n",
    "sgw_fs_operation_messages = [message[1] for message in dataset]\n",
    "ccp_response_messages = [message[2] for message in dataset]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'Executed Trade': ccp_new_trade_messages,\n",
    "  'Full Service Operation': sgw_fs_operation_messages,\n",
    "  'CCP Response': ccp_response_messages\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CCP Response</th>\n",
       "      <th>input_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"131755\" TrdID=\"36410189235\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"42663\" TrdID=\"28319621580\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"317405\" TrdID=\"23327228661\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...</td>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"86846\" TrdID=\"39125469483\" ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CCP Response  \\\n",
       "0  <TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...   \n",
       "1  <TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...   \n",
       "2  <TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...   \n",
       "3  <TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...   \n",
       "4  <TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...   \n",
       "\n",
       "                                       input_message  \n",
       "0  <TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" ...  \n",
       "1  <TrdCaptRpt RptID=\"131755\" TrdID=\"36410189235\"...  \n",
       "2  <TrdCaptRpt RptID=\"42663\" TrdID=\"28319621580\" ...  \n",
       "3  <TrdCaptRpt RptID=\"317405\" TrdID=\"23327228661\"...  \n",
       "4  <TrdCaptRpt RptID=\"86846\" TrdID=\"39125469483\" ...  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate input messages in a single one\n",
    "df['input_message'] = df['Executed Trade'] + ' ' + df['Full Service Operation']\n",
    "df.drop(['Executed Trade', 'Full Service Operation'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<TrdCaptRpt RptID=\"63271\" TrdID=\"31270713000\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"29\" MtchID=\"37713690660\" PxTyp=\"10\" LastQty=\"12\" LastPx=\"250.45\" TrdDt=\"2022-12-21\" BizDt=\"2023-02-10\" TxnTm=\"2023-02-15T12:25:13.230-5:00\"><Hdr SID=\"MGEX\" TID=\"D210\" PosDup=\"N\" PosRsnd=\"Y\" Snt=\"2023-02-15T12:25:13.768-5:00\"></Hdr><Instrmt Sym=\"ABP\" ID=\"AS\" Src=\"H\" CFI=\"APYBKL\" MMY=\"202310\" MatDt=\"2023-10-2\" Mult=\"50\" Exch=\"SNTL\"></Instrmt><RptSide Side=\"1\" Ccy=\"USD\" InptSrc=\"SY\" InptDev=\"API\" CustCpcty=\"1\" PosEfct=\"C\" ClOrdID=\"OrdAT\" MLegRptTyp=\"1\" AllocInd=\"0\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"500\" R=\"4\"></Pty><Pty ID=\"ahmed\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"></Sub></Pty><Pty ID=\"A123\" R=\"12\"></Pty><TrdRegTS TS=\"2023-02-15T12:25:13.230-5:00\" Typ=\"1\"></TrdRegTS></RptSide></TrdCaptRpt> <TrdCaptRpt RptID=\"31270713000\" TrdDt=\"2022-12-21\" BizDt=\"2023-02-10\" TxnTm=\"2023-02-15T12:25:13.230-5:00\" TrdID=\"31270713000\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"12.0\" LastPx=\"250.45\"><Hdr Snt=\"2023-02-15T17:30:55.600+01:00\" TID=\"MGEX\" SID=\"D210\"/><Instrmt Exch=\"SNTL\" ID=\"AS\" MMY=\"202310\" CFI=\"MKLPON\"/><RptSide Side=\"1\" CustCpcty=\"2\"><Pty ID=\"210\" R=\"1\"/><Pty ID=\"MPLAMP6\" R=\"24\"><Sub ID=\"2\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input_message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_message):\n",
    "  # input_message = re.sub(r'<[^>]*>', '', input_message)\n",
    "  # input_message = re.sub(r'\\s+','', input_message)\n",
    "  pattern = r'[<\\\"/]' # Define a regex pattern to match any string that starts with '<', or, '\"', or '/'. The bracket [...] means any character that is listed inside the bracekt\n",
    "  cleaned_text1 = re.sub(pattern, '', input_message) # re.sub will replace all matches of the regex pattern with an empty string.\n",
    "  cleaned_text = cleaned_text1.replace('>',' ') # Finish the xml input seq clean-up by removing the remaining '>' character.\n",
    "  return cleaned_text\n",
    "  # return input_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrdCaptRpt RptID=63271 TrdID=31270713000 TransTyp=0 RptTyp=2 TrdTyp=29 MtchID=37713690660 PxTyp=10 LastQty=12 LastPx=250.45 TrdDt=2022-12-21 BizDt=2023-02-10 TxnTm=2023-02-15T12:25:13.230-5:00 Hdr SID=MGEX TID=D210 PosDup=N PosRsnd=Y Snt=2023-02-15T12:25:13.768-5:00 Hdr Instrmt Sym=ABP ID=AS Src=H CFI=APYBKL MMY=202310 MatDt=2023-10-2 Mult=50 Exch=SNTL Instrmt RptSide Side=1 Ccy=USD InptSrc=SY InptDev=API CustCpcty=1 PosEfct=C ClOrdID=OrdAT MLegRptTyp=1 AllocInd=0 Pty ID=MGEX R=21 Pty Pty ID=210 R=1 Pty Pty ID=500 R=4 Pty Pty ID=ahmed R=24 Sub ID=2 Typ=26 Sub Pty Pty ID=A123 R=12 Pty TrdRegTS TS=2023-02-15T12:25:13.230-5:00 Typ=1 TrdRegTS RptSide TrdCaptRpt  TrdCaptRpt RptID=31270713000 TrdDt=2022-12-21 BizDt=2023-02-10 TxnTm=2023-02-15T12:25:13.230-5:00 TrdID=31270713000 TransTyp=2 RptTyp=0 LastQty=12.0 LastPx=250.45 Hdr Snt=2023-02-15T17:30:55.600+01:00 TID=MGEX SID=D210 Instrmt Exch=SNTL ID=AS MMY=202310 CFI=MKLPON RptSide Side=1 CustCpcty=2 Pty ID=210 R=1 Pty ID=MPLAMP6 R=24 Sub ID=2 Typ=26 Pty RptSide TrdCaptRpt \n",
      "1033\n"
     ]
    }
   ],
   "source": [
    "df['input_message_clean'] = df['input_message'].apply(clean_text)\n",
    "df.drop(['input_message'], axis=1, inplace=True)\n",
    "print(df['input_message_clean'][0])\n",
    "print(len(df['input_message_clean'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 100)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df.iloc[:int(len(df) * 0.9)] # Return the first 900 observations of a 1000 length dataset\n",
    "val_data = df.iloc[int(len(df) * 0.9):] # Return the last 100 observations of a 1000 length dataset\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 2)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CCP Response</th>\n",
       "      <th>input_message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...</td>\n",
       "      <td>TrdCaptRpt RptID=63271 TrdID=31270713000 Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...</td>\n",
       "      <td>TrdCaptRpt RptID=131755 TrdID=36410189235 Tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...</td>\n",
       "      <td>TrdCaptRpt RptID=42663 TrdID=28319621580 Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...</td>\n",
       "      <td>TrdCaptRpt RptID=317405 TrdID=23327228661 Tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...</td>\n",
       "      <td>TrdCaptRpt RptID=86846 TrdID=39125469483 Trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        CCP Response  \\\n",
       "0  <TrdCaptRpt RptID=\"580198\" TrdID=\"31270713000\"...   \n",
       "1  <TrdCaptRpt RptID=\"580198\" TrdID=\"36410189235\"...   \n",
       "2  <TrdCaptRpt RptID=\"580198\" TrdID=\"28319621580\"...   \n",
       "3  <TrdCaptRpt RptID=\"580198\" TrdID=\"23327228661\"...   \n",
       "4  <TrdCaptRpt RptID=\"580198\" TrdID=\"39125469483\"...   \n",
       "\n",
       "                                 input_message_clean  \n",
       "0  TrdCaptRpt RptID=63271 TrdID=31270713000 Trans...  \n",
       "1  TrdCaptRpt RptID=131755 TrdID=36410189235 Tran...  \n",
       "2  TrdCaptRpt RptID=42663 TrdID=28319621580 Trans...  \n",
       "3  TrdCaptRpt RptID=317405 TrdID=23327228661 Tran...  \n",
       "4  TrdCaptRpt RptID=86846 TrdID=39125469483 Trans...  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, max_length):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length # The maximum number of tokens\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data) # The number of samples in the dataset\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    input_seq = self.data.iloc[index]['input_message_clean']g\n",
    "    target_seq = self.data.iloc[index]['CCP Response']\n",
    "\n",
    "    concat_seq = f\"{input_seq} {target_seq}\" # Concatenate the input and the target message\n",
    "\n",
    "    tokenized_seq = self.tokenizer.encode(\n",
    "      concat_seq, \n",
    "      return_tensors='pt', # Return pytorch tensor for the tokenized sequence dataset\n",
    "      max_length=self.max_length, # Max padding length\n",
    "      truncation=True,\n",
    "      padding='max_length'\n",
    "    )\n",
    "    \n",
    "    return tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only needed when working from local environment, due to local SSL security when loading the HuggingFace model and token below. Remove this when working from Colab or Kaggle for example\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2') # Initialize using the distilgpt2 pre-trained model tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2') # Initialize the GPT2LMHeadModel with the distilgpt2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the remaining positions in the input sequence to the end of sentence token (EOS) if the sequence is shorter than the max_length \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1000 # Max number of tokens\n",
    "BATCH_SIZE = 2 # Load from the dataset in a batch way; the quicker and recommended way for performance reasons \n",
    "EPOCHS = 3 # Number of training passes ( i.e. Number of Forrward/Backward propagation cycles to reduce the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and validation datasets\n",
    "train_data_tokenized = CustomDataset(data=train_data, tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "val_data_tokenized = CustomDataset(data=val_data, tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data_tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for the training dataset\n",
    "# Batches data according to BATCH_SIZE\n",
    "# Shuffles the data to randomize the order of samples for better generalization during training\n",
    "train_data_loader = DataLoader(train_data_tokenized, batch_size= BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data_tokenized, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a CUDA-enabled GPU is enabled, otherwise use the default system CPU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device) # Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Torch optimizer for the model training\n",
    "# Use AdamW optimizer\n",
    "# With a learning rate of 2e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "Loss: 3.7932\n",
      "Loss: 3.5653\n",
      "Loss: 3.4803\n",
      "Loss: 3.3300\n",
      "Loss: 3.1834\n",
      "Loss: 3.0571\n",
      "Loss: 2.9807\n",
      "Loss: 2.8959\n",
      "Loss: 2.8205\n",
      "Loss: 2.7569\n",
      "Loss: 2.6836\n",
      "Loss: 2.6523\n",
      "Loss: 2.5788\n",
      "Loss: 2.4582\n",
      "Loss: 2.4112\n",
      "Loss: 2.3508\n",
      "Loss: 2.3196\n",
      "Loss: 2.2947\n",
      "Loss: 2.2927\n",
      "Loss: 2.2140\n",
      "Loss: 2.2049\n",
      "Loss: 2.1100\n",
      "Loss: 2.1298\n",
      "Loss: 2.1332\n",
      "Loss: 2.0482\n",
      "Loss: 2.0495\n",
      "Loss: 2.0315\n",
      "Loss: 2.0200\n",
      "Loss: 1.9859\n",
      "Loss: 1.9672\n",
      "Loss: 1.9808\n",
      "Loss: 1.9324\n",
      "Loss: 1.9244\n",
      "Loss: 1.8833\n",
      "Loss: 1.8621\n",
      "Loss: 1.8378\n",
      "Loss: 1.8524\n",
      "Loss: 1.8172\n",
      "Loss: 1.8179\n",
      "Loss: 1.7937\n",
      "Loss: 1.7499\n",
      "Loss: 1.7392\n",
      "Loss: 1.7237\n",
      "Loss: 1.7054\n",
      "Loss: 1.6537\n",
      "Loss: 1.6782\n",
      "Loss: 1.6525\n",
      "Loss: 1.6136\n",
      "Loss: 1.6140\n",
      "Loss: 1.5784\n",
      "Loss: 1.5605\n",
      "Loss: 1.5805\n",
      "Loss: 1.5486\n",
      "Loss: 1.5344\n",
      "Loss: 1.4892\n",
      "Loss: 1.4770\n",
      "Loss: 1.4245\n",
      "Loss: 1.4539\n",
      "Loss: 1.4484\n",
      "Loss: 1.3919\n",
      "Loss: 1.4313\n",
      "Loss: 1.3763\n",
      "Loss: 1.3580\n",
      "Loss: 1.2897\n",
      "Loss: 1.3808\n",
      "Loss: 1.2878\n",
      "Loss: 1.2843\n",
      "Loss: 1.2762\n",
      "Loss: 1.2987\n",
      "Loss: 1.1964\n",
      "Loss: 1.2286\n",
      "Loss: 1.1974\n",
      "Loss: 1.1758\n",
      "Loss: 1.1822\n",
      "Loss: 1.1588\n",
      "Loss: 1.1411\n",
      "Loss: 1.1632\n",
      "Loss: 1.1083\n",
      "Loss: 1.0769\n",
      "Loss: 1.0679\n",
      "Loss: 1.0921\n",
      "Loss: 1.0653\n",
      "Loss: 1.0371\n",
      "Loss: 1.0473\n",
      "Loss: 0.9706\n",
      "Loss: 1.0071\n",
      "Loss: 0.9745\n",
      "Loss: 0.9894\n",
      "Loss: 0.9580\n",
      "Loss: 0.9663\n",
      "Loss: 0.9467\n",
      "Loss: 0.8894\n",
      "Loss: 0.9632\n",
      "Loss: 0.8991\n",
      "Loss: 0.8805\n",
      "Loss: 0.8636\n",
      "Loss: 0.8872\n",
      "Loss: 0.8561\n",
      "Loss: 0.8396\n",
      "Loss: 0.8307\n",
      "Loss: 0.8323\n",
      "Loss: 0.8277\n",
      "Loss: 0.8379\n",
      "Loss: 0.7974\n",
      "Loss: 0.8130\n",
      "Loss: 0.8178\n",
      "Loss: 0.8152\n",
      "Loss: 0.7438\n",
      "Loss: 0.7411\n",
      "Loss: 0.7390\n",
      "Loss: 0.7607\n",
      "Loss: 0.7304\n",
      "Loss: 0.6984\n",
      "Loss: 0.6913\n",
      "Loss: 0.7228\n",
      "Loss: 0.7001\n",
      "Loss: 0.6636\n",
      "Loss: 0.6508\n",
      "Loss: 0.6892\n",
      "Loss: 0.6555\n",
      "Loss: 0.6568\n",
      "Loss: 0.6523\n",
      "Loss: 0.6414\n",
      "Loss: 0.6386\n",
      "Loss: 0.6408\n",
      "Loss: 0.6510\n",
      "Loss: 0.6270\n",
      "Loss: 0.6460\n",
      "Loss: 0.6050\n",
      "Loss: 0.6040\n",
      "Loss: 0.5837\n",
      "Loss: 0.6086\n",
      "Loss: 0.5769\n",
      "Loss: 0.5781\n",
      "Loss: 0.5824\n",
      "Loss: 0.5744\n",
      "Loss: 0.5380\n",
      "Loss: 0.5644\n",
      "Loss: 0.5484\n",
      "Loss: 0.5336\n",
      "Loss: 0.5113\n",
      "Loss: 0.5530\n",
      "Loss: 0.5389\n",
      "Loss: 0.5458\n",
      "Loss: 0.5139\n",
      "Loss: 0.5085\n",
      "Loss: 0.5260\n",
      "Loss: 0.4968\n",
      "Loss: 0.5206\n",
      "Loss: 0.5014\n",
      "Loss: 0.4988\n",
      "Loss: 0.4848\n",
      "Loss: 0.5048\n",
      "Loss: 0.4562\n",
      "Loss: 0.4941\n",
      "Loss: 0.4785\n",
      "Loss: 0.4790\n",
      "Loss: 0.4811\n",
      "Loss: 0.4756\n",
      "Loss: 0.4658\n",
      "Loss: 0.4456\n",
      "Loss: 0.4532\n",
      "Loss: 0.4718\n",
      "Loss: 0.4416\n",
      "Loss: 0.4407\n",
      "Loss: 0.4309\n",
      "Loss: 0.4594\n",
      "Loss: 0.4145\n",
      "Loss: 0.4602\n",
      "Loss: 0.4351\n",
      "Loss: 0.4194\n",
      "Loss: 0.4502\n",
      "Loss: 0.4152\n",
      "Loss: 0.4337\n",
      "Loss: 0.4265\n",
      "Loss: 0.4330\n",
      "Loss: 0.4227\n",
      "Loss: 0.4173\n",
      "Loss: 0.3970\n",
      "Loss: 0.4008\n",
      "Loss: 0.3950\n",
      "Loss: 0.3881\n",
      "Loss: 0.3979\n",
      "Loss: 0.3955\n",
      "Loss: 0.3773\n",
      "Loss: 0.3825\n",
      "Loss: 0.3893\n",
      "Loss: 0.4097\n",
      "Loss: 0.3974\n",
      "Loss: 0.3673\n",
      "Loss: 0.3980\n",
      "Loss: 0.3893\n",
      "Loss: 0.3750\n",
      "Loss: 0.3621\n",
      "Loss: 0.3786\n",
      "Loss: 0.3809\n",
      "Loss: 0.3612\n",
      "Loss: 0.3628\n",
      "Loss: 0.4049\n",
      "Loss: 0.3583\n",
      "Loss: 0.3669\n",
      "Loss: 0.3593\n",
      "Loss: 0.3264\n",
      "Loss: 0.3476\n",
      "Loss: 0.3530\n",
      "Loss: 0.3358\n",
      "Loss: 0.3714\n",
      "Loss: 0.3413\n",
      "Loss: 0.3457\n",
      "Loss: 0.3474\n",
      "Loss: 0.3314\n",
      "Loss: 0.3355\n",
      "Loss: 0.3149\n",
      "Loss: 0.3431\n",
      "Loss: 0.3128\n",
      "Loss: 0.3460\n",
      "Loss: 0.3113\n",
      "Loss: 0.3317\n",
      "Loss: 0.3187\n",
      "Loss: 0.3309\n",
      "Loss: 0.3493\n",
      "Loss: 0.3247\n",
      "Loss: 0.3399\n",
      "Loss: 0.3247\n",
      "Loss: 0.3323\n",
      "Loss: 0.3231\n",
      "Loss: 0.3238\n",
      "Loss: 0.3029\n",
      "Loss: 0.3139\n",
      "Loss: 0.3117\n",
      "Loss: 0.3070\n",
      "Loss: 0.3063\n",
      "Loss: 0.3229\n",
      "Loss: 0.3250\n",
      "Loss: 0.3130\n",
      "Loss: 0.3103\n",
      "Loss: 0.3189\n",
      "Loss: 0.3143\n",
      "Loss: 0.3054\n",
      "Loss: 0.3236\n",
      "Loss: 0.2794\n",
      "Loss: 0.3112\n",
      "Loss: 0.3074\n",
      "Loss: 0.2844\n",
      "Loss: 0.2950\n",
      "Loss: 0.3061\n",
      "Loss: 0.2949\n",
      "Loss: 0.2882\n",
      "Loss: 0.2851\n",
      "Loss: 0.2915\n",
      "Loss: 0.2940\n",
      "Loss: 0.3045\n",
      "Loss: 0.3089\n",
      "Loss: 0.2864\n",
      "Loss: 0.2964\n",
      "Loss: 0.3085\n",
      "Loss: 0.2913\n",
      "Loss: 0.2896\n",
      "Loss: 0.2966\n",
      "Loss: 0.3004\n",
      "Loss: 0.2869\n",
      "Loss: 0.2724\n",
      "Loss: 0.2897\n",
      "Loss: 0.2958\n",
      "Loss: 0.2733\n",
      "Loss: 0.2854\n",
      "Loss: 0.2695\n",
      "Loss: 0.3112\n",
      "Loss: 0.2618\n",
      "Loss: 0.2831\n",
      "Loss: 0.2686\n",
      "Loss: 0.2858\n",
      "Loss: 0.2741\n",
      "Loss: 0.2954\n",
      "Loss: 0.2791\n",
      "Loss: 0.2788\n",
      "Loss: 0.2624\n",
      "Loss: 0.2787\n",
      "Loss: 0.2587\n",
      "Loss: 0.2536\n",
      "Loss: 0.2631\n",
      "Loss: 0.2596\n",
      "Loss: 0.2779\n",
      "Loss: 0.2544\n",
      "Loss: 0.2646\n",
      "Loss: 0.2766\n",
      "Loss: 0.2609\n",
      "Loss: 0.2630\n",
      "Loss: 0.2519\n",
      "Loss: 0.2594\n",
      "Loss: 0.2536\n",
      "Loss: 0.2513\n",
      "Loss: 0.2668\n",
      "Loss: 0.2564\n",
      "Loss: 0.2505\n",
      "Loss: 0.2575\n",
      "Loss: 0.2486\n",
      "Loss: 0.2474\n",
      "Loss: 0.2641\n",
      "Loss: 0.2470\n",
      "Loss: 0.2491\n",
      "Loss: 0.2512\n",
      "Loss: 0.2504\n",
      "Loss: 0.2645\n",
      "Loss: 0.2689\n",
      "Loss: 0.2487\n",
      "Loss: 0.2478\n",
      "Loss: 0.2507\n",
      "Loss: 0.2395\n",
      "Loss: 0.2550\n",
      "Loss: 0.2456\n",
      "Loss: 0.2433\n",
      "Loss: 0.2497\n",
      "Loss: 0.2574\n",
      "Loss: 0.2497\n",
      "Loss: 0.2510\n",
      "Loss: 0.2373\n",
      "Loss: 0.2607\n",
      "Loss: 0.2355\n",
      "Loss: 0.2466\n",
      "Loss: 0.2354\n",
      "Loss: 0.2652\n",
      "Loss: 0.2407\n",
      "Loss: 0.2452\n",
      "Loss: 0.2363\n",
      "Loss: 0.2376\n",
      "Loss: 0.2266\n",
      "Loss: 0.2377\n",
      "Loss: 0.2441\n",
      "Loss: 0.2320\n",
      "Loss: 0.2301\n",
      "Loss: 0.2329\n",
      "Loss: 0.2445\n",
      "Loss: 0.2586\n",
      "Loss: 0.2373\n",
      "Loss: 0.2426\n",
      "Loss: 0.2459\n",
      "Loss: 0.2374\n",
      "Loss: 0.2261\n",
      "Loss: 0.2537\n",
      "Loss: 0.2335\n",
      "Loss: 0.2313\n",
      "Loss: 0.2416\n",
      "Loss: 0.2249\n",
      "Loss: 0.2380\n",
      "Loss: 0.2349\n",
      "Loss: 0.2481\n",
      "Loss: 0.2365\n",
      "Loss: 0.2419\n",
      "Loss: 0.2402\n",
      "Loss: 0.2327\n",
      "Loss: 0.2286\n",
      "Loss: 0.2367\n",
      "Loss: 0.2290\n",
      "Loss: 0.2259\n",
      "Loss: 0.2285\n",
      "Loss: 0.2507\n",
      "Loss: 0.2208\n",
      "Loss: 0.2374\n",
      "Loss: 0.2293\n",
      "Loss: 0.2251\n",
      "Loss: 0.2191\n",
      "Loss: 0.2175\n",
      "Loss: 0.2341\n",
      "Loss: 0.2362\n",
      "Loss: 0.2293\n",
      "Loss: 0.2442\n",
      "Loss: 0.2189\n",
      "Loss: 0.2206\n",
      "Loss: 0.2300\n",
      "Loss: 0.2259\n",
      "Loss: 0.2163\n",
      "Loss: 0.2236\n",
      "Loss: 0.2223\n",
      "Loss: 0.2163\n",
      "Loss: 0.2085\n",
      "Loss: 0.2394\n",
      "Loss: 0.2222\n",
      "Loss: 0.2261\n",
      "Loss: 0.2123\n",
      "Loss: 0.2228\n",
      "Loss: 0.2304\n",
      "Loss: 0.2329\n",
      "Loss: 0.2223\n",
      "Loss: 0.2176\n",
      "Loss: 0.2122\n",
      "Loss: 0.2204\n",
      "Loss: 0.2390\n",
      "Loss: 0.2196\n",
      "Loss: 0.2171\n",
      "Loss: 0.2081\n",
      "Loss: 0.2199\n",
      "Loss: 0.2129\n",
      "Loss: 0.2130\n",
      "Loss: 0.2164\n",
      "Loss: 0.2109\n",
      "Loss: 0.2056\n",
      "Loss: 0.2248\n",
      "Loss: 0.2244\n",
      "Loss: 0.2085\n",
      "Loss: 0.2269\n",
      "Loss: 0.2165\n",
      "Loss: 0.2288\n",
      "Loss: 0.2085\n",
      "Loss: 0.2122\n",
      "Loss: 0.1973\n",
      "Loss: 0.2151\n",
      "Loss: 0.2040\n",
      "Loss: 0.2088\n",
      "Loss: 0.2087\n",
      "Loss: 0.2079\n",
      "Loss: 0.2040\n",
      "Loss: 0.1992\n",
      "Loss: 0.2198\n",
      "Loss: 0.2044\n",
      "Loss: 0.2125\n",
      "Loss: 0.2224\n",
      "Loss: 0.2240\n",
      "Loss: 0.2001\n",
      "Loss: 0.2014\n",
      "Loss: 0.2098\n",
      "Loss: 0.2537\n",
      "Loss: 0.2117\n",
      "Loss: 0.1982\n",
      "Loss: 0.2156\n",
      "Loss: 0.3205\n",
      "Loss: 0.2203\n",
      "Loss: 0.2220\n",
      "Loss: 0.2041\n",
      "Loss: 0.2270\n",
      "Loss: 0.2208\n",
      "Loss: 0.2028\n",
      "Loss: 0.2041\n",
      "Loss: 0.2079\n",
      "Loss: 0.2095\n",
      "Loss: 0.1994\n",
      "Loss: 0.2117\n",
      "Loss: 0.2179\n",
      "Loss: 0.2110\n",
      "Loss: 0.2147\n",
      "Loss: 0.1986\n",
      "Loss: 0.2029\n",
      "Loss: 0.2147\n",
      "Loss: 0.2036\n",
      "Loss: 0.2107\n",
      "Loss: 0.2149\n",
      "Loss: 0.1961\n",
      "Loss: 0.1991\n",
      "Loss: 0.2014\n",
      "Loss: 0.2005\n",
      "Epoch 1 Average Training Loss: 0.6399\n",
      "----------\n",
      "Epoch 2/3\n",
      "----------\n",
      "Loss: 0.2048\n",
      "Loss: 0.1909\n",
      "Loss: 0.2114\n",
      "Loss: 0.1943\n",
      "Loss: 0.2098\n",
      "Loss: 0.2017\n",
      "Loss: 0.2066\n",
      "Loss: 0.2002\n",
      "Loss: 0.2037\n",
      "Loss: 0.1981\n",
      "Loss: 0.1949\n",
      "Loss: 0.1938\n",
      "Loss: 0.2155\n",
      "Loss: 0.1902\n",
      "Loss: 0.1962\n",
      "Loss: 0.2043\n",
      "Loss: 0.1871\n",
      "Loss: 0.1925\n",
      "Loss: 0.1875\n",
      "Loss: 0.2143\n",
      "Loss: 0.1965\n",
      "Loss: 0.1894\n",
      "Loss: 0.1824\n",
      "Loss: 0.1851\n",
      "Loss: 0.2165\n",
      "Loss: 0.1969\n",
      "Loss: 0.1872\n",
      "Loss: 0.2074\n",
      "Loss: 0.1982\n",
      "Loss: 0.2014\n",
      "Loss: 0.2057\n",
      "Loss: 0.1838\n",
      "Loss: 0.1992\n",
      "Loss: 0.1970\n",
      "Loss: 0.1977\n",
      "Loss: 0.1972\n",
      "Loss: 0.2095\n",
      "Loss: 0.1892\n",
      "Loss: 0.2008\n",
      "Loss: 0.2045\n",
      "Loss: 0.1909\n",
      "Loss: 0.1823\n",
      "Loss: 0.1990\n",
      "Loss: 0.1942\n",
      "Loss: 0.1957\n",
      "Loss: 0.2004\n",
      "Loss: 0.1864\n",
      "Loss: 0.1869\n",
      "Loss: 0.2006\n",
      "Loss: 0.1848\n",
      "Loss: 0.1899\n",
      "Loss: 0.1945\n",
      "Loss: 0.2097\n",
      "Loss: 0.1846\n",
      "Loss: 0.1981\n",
      "Loss: 0.1809\n",
      "Loss: 0.1899\n",
      "Loss: 0.1881\n",
      "Loss: 0.1822\n",
      "Loss: 0.1971\n",
      "Loss: 0.1855\n",
      "Loss: 0.2005\n",
      "Loss: 0.1936\n",
      "Loss: 0.1959\n",
      "Loss: 0.1828\n",
      "Loss: 0.1850\n",
      "Loss: 0.1922\n",
      "Loss: 0.1797\n",
      "Loss: 0.1909\n",
      "Loss: 0.1859\n",
      "Loss: 0.1885\n",
      "Loss: 0.1785\n",
      "Loss: 0.1966\n",
      "Loss: 0.1856\n",
      "Loss: 0.2027\n",
      "Loss: 0.1895\n",
      "Loss: 0.1904\n",
      "Loss: 0.1947\n",
      "Loss: 0.1836\n",
      "Loss: 0.1922\n",
      "Loss: 0.1986\n",
      "Loss: 0.1908\n",
      "Loss: 0.1854\n",
      "Loss: 0.1826\n",
      "Loss: 0.2131\n",
      "Loss: 0.1898\n",
      "Loss: 0.1994\n",
      "Loss: 0.1902\n",
      "Loss: 0.2133\n",
      "Loss: 0.1863\n",
      "Loss: 0.1935\n",
      "Loss: 0.1850\n",
      "Loss: 0.1749\n",
      "Loss: 0.1810\n",
      "Loss: 0.1995\n",
      "Loss: 0.1939\n",
      "Loss: 0.1851\n",
      "Loss: 0.2016\n",
      "Loss: 0.1893\n",
      "Loss: 0.1802\n",
      "Loss: 0.1996\n",
      "Loss: 0.1908\n",
      "Loss: 0.1820\n",
      "Loss: 0.1756\n",
      "Loss: 0.1857\n",
      "Loss: 0.1904\n",
      "Loss: 0.1934\n",
      "Loss: 0.1827\n",
      "Loss: 0.1796\n",
      "Loss: 0.1887\n",
      "Loss: 0.1833\n",
      "Loss: 0.1875\n",
      "Loss: 0.1823\n",
      "Loss: 0.1909\n",
      "Loss: 0.1735\n",
      "Loss: 0.1946\n",
      "Loss: 0.1837\n",
      "Loss: 0.1870\n",
      "Loss: 0.1867\n",
      "Loss: 0.1851\n",
      "Loss: 0.1801\n",
      "Loss: 0.1866\n",
      "Loss: 0.2029\n",
      "Loss: 0.1813\n",
      "Loss: 0.1813\n",
      "Loss: 0.1873\n",
      "Loss: 0.1834\n",
      "Loss: 0.1787\n",
      "Loss: 0.1848\n",
      "Loss: 0.1879\n",
      "Loss: 0.1779\n",
      "Loss: 0.1698\n",
      "Loss: 0.1767\n",
      "Loss: 0.1841\n",
      "Loss: 0.1862\n",
      "Loss: 0.1764\n",
      "Loss: 0.1722\n",
      "Loss: 0.1741\n",
      "Loss: 0.1798\n",
      "Loss: 0.1827\n",
      "Loss: 0.1905\n",
      "Loss: 0.1837\n",
      "Loss: 0.1748\n",
      "Loss: 0.1831\n",
      "Loss: 0.1957\n",
      "Loss: 0.1864\n",
      "Loss: 0.1867\n",
      "Loss: 0.1895\n",
      "Loss: 0.1848\n",
      "Loss: 0.1741\n",
      "Loss: 0.1878\n",
      "Loss: 0.1832\n",
      "Loss: 0.1845\n",
      "Loss: 0.1687\n",
      "Loss: 0.1792\n",
      "Loss: 0.1824\n",
      "Loss: 0.1809\n",
      "Loss: 0.1868\n",
      "Loss: 0.1879\n",
      "Loss: 0.1764\n",
      "Loss: 0.1789\n",
      "Loss: 0.1739\n",
      "Loss: 0.1821\n",
      "Loss: 0.1736\n",
      "Loss: 0.1709\n",
      "Loss: 0.1788\n",
      "Loss: 0.1783\n",
      "Loss: 0.1786\n",
      "Loss: 0.1713\n",
      "Loss: 0.1779\n",
      "Loss: 0.1842\n",
      "Loss: 0.1787\n",
      "Loss: 0.1768\n",
      "Loss: 0.1826\n",
      "Loss: 0.1776\n",
      "Loss: 0.1730\n",
      "Loss: 0.1843\n",
      "Loss: 0.1778\n",
      "Loss: 0.1867\n",
      "Loss: 0.1828\n",
      "Loss: 0.1775\n",
      "Loss: 0.1662\n",
      "Loss: 0.1745\n",
      "Loss: 0.1750\n",
      "Loss: 0.1766\n",
      "Loss: 0.1802\n",
      "Loss: 0.1716\n",
      "Loss: 0.1763\n",
      "Loss: 0.1837\n",
      "Loss: 0.1876\n",
      "Loss: 0.1896\n",
      "Loss: 0.1705\n",
      "Loss: 0.1928\n",
      "Loss: 0.1715\n",
      "Loss: 0.1653\n",
      "Loss: 0.1802\n",
      "Loss: 0.1768\n",
      "Loss: 0.1873\n",
      "Loss: 0.1737\n",
      "Loss: 0.1740\n",
      "Loss: 0.1884\n",
      "Loss: 0.1710\n",
      "Loss: 0.1894\n",
      "Loss: 0.1824\n",
      "Loss: 0.1809\n",
      "Loss: 0.1761\n",
      "Loss: 0.1689\n",
      "Loss: 0.1686\n",
      "Loss: 0.1654\n",
      "Loss: 0.1758\n",
      "Loss: 0.1803\n",
      "Loss: 0.1814\n",
      "Loss: 0.1866\n",
      "Loss: 0.1716\n",
      "Loss: 0.1666\n",
      "Loss: 0.1808\n",
      "Loss: 0.1694\n",
      "Loss: 0.1725\n",
      "Loss: 0.1934\n",
      "Loss: 0.1882\n",
      "Loss: 0.1788\n",
      "Loss: 0.1739\n",
      "Loss: 0.1831\n",
      "Loss: 0.1781\n",
      "Loss: 0.1690\n",
      "Loss: 0.1759\n",
      "Loss: 0.1660\n",
      "Loss: 0.1884\n",
      "Loss: 0.1745\n",
      "Loss: 0.1685\n",
      "Loss: 0.1837\n",
      "Loss: 0.1856\n",
      "Loss: 0.1652\n",
      "Loss: 0.1755\n",
      "Loss: 0.1899\n",
      "Loss: 0.1909\n",
      "Loss: 0.1728\n",
      "Loss: 0.1830\n",
      "Loss: 0.1749\n",
      "Loss: 0.1713\n",
      "Loss: 0.1705\n",
      "Loss: 0.1732\n",
      "Loss: 0.1600\n",
      "Loss: 0.1688\n",
      "Loss: 0.1715\n",
      "Loss: 0.1717\n",
      "Loss: 0.1653\n",
      "Loss: 0.1752\n",
      "Loss: 0.1717\n",
      "Loss: 0.1720\n",
      "Loss: 0.1767\n",
      "Loss: 0.1650\n",
      "Loss: 0.1576\n",
      "Loss: 0.1805\n",
      "Loss: 0.1697\n",
      "Loss: 0.1606\n",
      "Loss: 0.1793\n",
      "Loss: 0.1752\n",
      "Loss: 0.1724\n",
      "Loss: 0.1641\n",
      "Loss: 0.1775\n",
      "Loss: 0.1780\n",
      "Loss: 0.1722\n",
      "Loss: 0.1796\n",
      "Loss: 0.1784\n",
      "Loss: 0.1678\n",
      "Loss: 0.1741\n",
      "Loss: 0.1595\n",
      "Loss: 0.1778\n",
      "Loss: 0.1833\n",
      "Loss: 0.1745\n",
      "Loss: 0.1763\n",
      "Loss: 0.1666\n",
      "Loss: 0.1831\n",
      "Loss: 0.1706\n",
      "Loss: 0.1654\n",
      "Loss: 0.1701\n",
      "Loss: 0.1806\n",
      "Loss: 0.1747\n",
      "Loss: 0.1667\n",
      "Loss: 0.1753\n",
      "Loss: 0.1736\n",
      "Loss: 0.1634\n",
      "Loss: 0.1736\n",
      "Loss: 0.1800\n",
      "Loss: 0.1690\n",
      "Loss: 0.1707\n",
      "Loss: 0.1803\n",
      "Loss: 0.1679\n",
      "Loss: 0.1708\n",
      "Loss: 0.1671\n",
      "Loss: 0.1716\n",
      "Loss: 0.1680\n",
      "Loss: 0.1748\n",
      "Loss: 0.1675\n",
      "Loss: 0.1692\n",
      "Loss: 0.1724\n",
      "Loss: 0.1680\n",
      "Loss: 0.1666\n",
      "Loss: 0.1749\n",
      "Loss: 0.1716\n",
      "Loss: 0.1749\n",
      "Loss: 0.1734\n",
      "Loss: 0.1620\n",
      "Loss: 0.1731\n",
      "Loss: 0.1719\n",
      "Loss: 0.1702\n",
      "Loss: 0.1683\n",
      "Loss: 0.1670\n",
      "Loss: 0.1693\n",
      "Loss: 0.1701\n",
      "Loss: 0.1514\n",
      "Loss: 0.1611\n",
      "Loss: 0.1727\n",
      "Loss: 0.1679\n",
      "Loss: 0.1677\n",
      "Loss: 0.1684\n",
      "Loss: 0.1649\n",
      "Loss: 0.1688\n",
      "Loss: 0.1708\n",
      "Loss: 0.1701\n",
      "Loss: 0.1655\n",
      "Loss: 0.1671\n",
      "Loss: 0.1746\n",
      "Loss: 0.1703\n",
      "Loss: 0.1714\n",
      "Loss: 0.1851\n",
      "Loss: 0.1674\n",
      "Loss: 0.1762\n",
      "Loss: 0.1696\n",
      "Loss: 0.1609\n",
      "Loss: 0.1698\n",
      "Loss: 0.1720\n",
      "Loss: 0.1678\n",
      "Loss: 0.1567\n",
      "Loss: 0.1795\n",
      "Loss: 0.1714\n",
      "Loss: 0.1768\n",
      "Loss: 0.1608\n",
      "Loss: 0.1881\n",
      "Loss: 0.1583\n",
      "Loss: 0.1708\n",
      "Loss: 0.1755\n",
      "Loss: 0.1781\n",
      "Loss: 0.1703\n",
      "Loss: 0.1737\n",
      "Loss: 0.1662\n",
      "Loss: 0.1587\n",
      "Loss: 0.1767\n",
      "Loss: 0.1623\n",
      "Loss: 0.1778\n",
      "Loss: 0.1623\n",
      "Loss: 0.1687\n",
      "Loss: 0.1702\n",
      "Loss: 0.1691\n",
      "Loss: 0.1684\n",
      "Loss: 0.1735\n",
      "Loss: 0.1697\n",
      "Loss: 0.1720\n",
      "Loss: 0.1715\n",
      "Loss: 0.1739\n",
      "Loss: 0.1657\n",
      "Loss: 0.1582\n",
      "Loss: 0.1620\n",
      "Loss: 0.1671\n",
      "Loss: 0.1619\n",
      "Loss: 0.1688\n",
      "Loss: 0.1574\n",
      "Loss: 0.1642\n",
      "Loss: 0.1683\n",
      "Loss: 0.1650\n",
      "Loss: 0.1667\n",
      "Loss: 0.1667\n",
      "Loss: 0.1720\n",
      "Loss: 0.1647\n",
      "Loss: 0.1638\n",
      "Loss: 0.1654\n",
      "Loss: 0.1698\n",
      "Loss: 0.1699\n",
      "Loss: 0.1578\n",
      "Loss: 0.1585\n",
      "Loss: 0.1578\n",
      "Loss: 0.1672\n",
      "Loss: 0.1757\n",
      "Loss: 0.1702\n",
      "Loss: 0.1609\n",
      "Loss: 0.1688\n",
      "Loss: 0.1795\n",
      "Loss: 0.1588\n",
      "Loss: 0.1684\n",
      "Loss: 0.1687\n",
      "Loss: 0.1767\n",
      "Loss: 0.1566\n",
      "Loss: 0.1751\n",
      "Loss: 0.1650\n",
      "Loss: 0.1571\n",
      "Loss: 0.1661\n",
      "Loss: 0.1651\n",
      "Loss: 0.1564\n",
      "Loss: 0.1645\n",
      "Loss: 0.1633\n",
      "Loss: 0.1611\n",
      "Loss: 0.1604\n",
      "Loss: 0.1645\n",
      "Loss: 0.1602\n",
      "Loss: 0.1569\n",
      "Loss: 0.1634\n",
      "Loss: 0.1598\n",
      "Loss: 0.1720\n",
      "Loss: 0.1578\n",
      "Loss: 0.1590\n",
      "Loss: 0.1631\n",
      "Loss: 0.1639\n",
      "Loss: 0.1646\n",
      "Loss: 0.1605\n",
      "Loss: 0.1640\n",
      "Loss: 0.1724\n",
      "Loss: 0.1592\n",
      "Loss: 0.1627\n",
      "Loss: 0.1637\n",
      "Loss: 0.1637\n",
      "Loss: 0.1746\n",
      "Loss: 0.1642\n",
      "Loss: 0.1690\n",
      "Loss: 0.1787\n",
      "Loss: 0.1579\n",
      "Loss: 0.1731\n",
      "Loss: 0.1732\n",
      "Loss: 0.1659\n",
      "Loss: 0.1666\n",
      "Loss: 0.1684\n",
      "Loss: 0.1730\n",
      "Loss: 0.1613\n",
      "Loss: 0.1673\n",
      "Loss: 0.1695\n",
      "Loss: 0.1616\n",
      "Loss: 0.1661\n",
      "Loss: 0.1715\n",
      "Loss: 0.1533\n",
      "Loss: 0.1649\n",
      "Loss: 0.1758\n",
      "Loss: 0.1702\n",
      "Loss: 0.1609\n",
      "Loss: 0.1768\n",
      "Loss: 0.1587\n",
      "Loss: 0.1683\n",
      "Loss: 0.1661\n",
      "Loss: 0.1650\n",
      "Loss: 0.1735\n",
      "Loss: 0.1567\n",
      "Epoch 2 Average Training Loss: 0.1779\n",
      "----------\n",
      "Epoch 3/3\n",
      "----------\n",
      "Loss: 0.1592\n",
      "Loss: 0.1676\n",
      "Loss: 0.1575\n",
      "Loss: 0.1536\n",
      "Loss: 0.1726\n",
      "Loss: 0.1725\n",
      "Loss: 0.1661\n",
      "Loss: 0.1547\n",
      "Loss: 0.1602\n",
      "Loss: 0.1615\n",
      "Loss: 0.1548\n",
      "Loss: 0.1559\n",
      "Loss: 0.1565\n",
      "Loss: 0.1727\n",
      "Loss: 0.1562\n",
      "Loss: 0.1712\n",
      "Loss: 0.1787\n",
      "Loss: 0.1504\n",
      "Loss: 0.1527\n",
      "Loss: 0.1649\n",
      "Loss: 0.1649\n",
      "Loss: 0.1635\n",
      "Loss: 0.1547\n",
      "Loss: 0.1843\n",
      "Loss: 0.1657\n",
      "Loss: 0.1619\n",
      "Loss: 0.1704\n",
      "Loss: 0.1549\n",
      "Loss: 0.1598\n",
      "Loss: 0.1649\n",
      "Loss: 0.1658\n",
      "Loss: 0.1702\n",
      "Loss: 0.1688\n",
      "Loss: 0.1800\n",
      "Loss: 0.1597\n",
      "Loss: 0.1634\n",
      "Loss: 0.1604\n",
      "Loss: 0.1762\n",
      "Loss: 0.1757\n",
      "Loss: 0.1567\n",
      "Loss: 0.1574\n",
      "Loss: 0.1603\n",
      "Loss: 0.1669\n",
      "Loss: 0.1658\n",
      "Loss: 0.1648\n",
      "Loss: 0.1632\n",
      "Loss: 0.1637\n",
      "Loss: 0.1606\n",
      "Loss: 0.1616\n",
      "Loss: 0.1708\n",
      "Loss: 0.1558\n",
      "Loss: 0.1552\n",
      "Loss: 0.1606\n",
      "Loss: 0.1660\n",
      "Loss: 0.1593\n",
      "Loss: 0.1523\n",
      "Loss: 0.1579\n",
      "Loss: 0.1539\n",
      "Loss: 0.1615\n",
      "Loss: 0.1624\n",
      "Loss: 0.1557\n",
      "Loss: 0.1573\n",
      "Loss: 0.1663\n",
      "Loss: 0.1527\n",
      "Loss: 0.1567\n",
      "Loss: 0.1602\n",
      "Loss: 0.1557\n",
      "Loss: 0.1560\n",
      "Loss: 0.1719\n",
      "Loss: 0.1564\n",
      "Loss: 0.1607\n",
      "Loss: 0.1594\n",
      "Loss: 0.1621\n",
      "Loss: 0.1647\n",
      "Loss: 0.1680\n",
      "Loss: 0.1580\n",
      "Loss: 0.1662\n",
      "Loss: 0.1567\n",
      "Loss: 0.1606\n",
      "Loss: 0.1606\n",
      "Loss: 0.1576\n",
      "Loss: 0.1505\n",
      "Loss: 0.1531\n",
      "Loss: 0.1555\n",
      "Loss: 0.1558\n",
      "Loss: 0.1622\n",
      "Loss: 0.1556\n",
      "Loss: 0.1591\n",
      "Loss: 0.1669\n",
      "Loss: 0.1614\n",
      "Loss: 0.1489\n",
      "Loss: 0.1726\n",
      "Loss: 0.1508\n",
      "Loss: 0.1577\n",
      "Loss: 0.1568\n",
      "Loss: 0.1534\n",
      "Loss: 0.1701\n",
      "Loss: 0.1579\n",
      "Loss: 0.1521\n",
      "Loss: 0.1636\n",
      "Loss: 0.1586\n",
      "Loss: 0.1628\n",
      "Loss: 0.1711\n",
      "Loss: 0.1613\n",
      "Loss: 0.1616\n",
      "Loss: 0.1510\n",
      "Loss: 0.1646\n",
      "Loss: 0.1571\n",
      "Loss: 0.1531\n",
      "Loss: 0.1734\n",
      "Loss: 0.1562\n",
      "Loss: 0.1662\n",
      "Loss: 0.1589\n",
      "Loss: 0.1578\n",
      "Loss: 0.1593\n",
      "Loss: 0.1650\n",
      "Loss: 0.1645\n",
      "Loss: 0.1683\n",
      "Loss: 0.1664\n",
      "Loss: 0.1573\n",
      "Loss: 0.1617\n",
      "Loss: 0.1587\n",
      "Loss: 0.1668\n",
      "Loss: 0.1639\n",
      "Loss: 0.1605\n",
      "Loss: 0.1577\n",
      "Loss: 0.1622\n",
      "Loss: 0.1604\n",
      "Loss: 0.1638\n",
      "Loss: 0.1534\n",
      "Loss: 0.1505\n",
      "Loss: 0.1515\n",
      "Loss: 0.1630\n",
      "Loss: 0.1628\n",
      "Loss: 0.1565\n",
      "Loss: 0.1546\n",
      "Loss: 0.1604\n",
      "Loss: 0.1536\n",
      "Loss: 0.1552\n",
      "Loss: 0.1600\n",
      "Loss: 0.1596\n",
      "Loss: 0.1486\n",
      "Loss: 0.1541\n",
      "Loss: 0.1546\n",
      "Loss: 0.1482\n",
      "Loss: 0.1591\n",
      "Loss: 0.1707\n",
      "Loss: 0.1627\n",
      "Loss: 0.1579\n",
      "Loss: 0.1541\n",
      "Loss: 0.1631\n",
      "Loss: 0.1583\n",
      "Loss: 0.1449\n",
      "Loss: 0.1571\n",
      "Loss: 0.1575\n",
      "Loss: 0.1598\n",
      "Loss: 0.1612\n",
      "Loss: 0.1545\n",
      "Loss: 0.1614\n",
      "Loss: 0.1688\n",
      "Loss: 0.1513\n",
      "Loss: 0.1580\n",
      "Loss: 0.1648\n",
      "Loss: 0.1604\n",
      "Loss: 0.1582\n",
      "Loss: 0.1587\n",
      "Loss: 0.1611\n",
      "Loss: 0.1568\n",
      "Loss: 0.1596\n",
      "Loss: 0.1712\n",
      "Loss: 0.1568\n",
      "Loss: 0.1554\n",
      "Loss: 0.1598\n",
      "Loss: 0.1567\n",
      "Loss: 0.1509\n",
      "Loss: 0.1513\n",
      "Loss: 0.1514\n",
      "Loss: 0.1542\n",
      "Loss: 0.1585\n",
      "Loss: 0.1564\n",
      "Loss: 0.1621\n",
      "Loss: 0.1678\n",
      "Loss: 0.1510\n",
      "Loss: 0.1567\n",
      "Loss: 0.1471\n",
      "Loss: 0.1600\n",
      "Loss: 0.1584\n",
      "Loss: 0.1663\n",
      "Loss: 0.1698\n",
      "Loss: 0.1549\n",
      "Loss: 0.1525\n",
      "Loss: 0.1625\n",
      "Loss: 0.1673\n",
      "Loss: 0.1624\n",
      "Loss: 0.1524\n",
      "Loss: 0.1514\n",
      "Loss: 0.1593\n",
      "Loss: 0.1667\n",
      "Loss: 0.1527\n",
      "Loss: 0.1617\n",
      "Loss: 0.1520\n",
      "Loss: 0.1674\n",
      "Loss: 0.1588\n",
      "Loss: 0.1502\n",
      "Loss: 0.1597\n",
      "Loss: 0.1540\n",
      "Loss: 0.1611\n",
      "Loss: 0.1561\n",
      "Loss: 0.1598\n",
      "Loss: 0.1571\n",
      "Loss: 0.1664\n",
      "Loss: 0.1593\n",
      "Loss: 0.1556\n",
      "Loss: 0.1519\n",
      "Loss: 0.1633\n",
      "Loss: 0.1588\n",
      "Loss: 0.1590\n",
      "Loss: 0.1512\n",
      "Loss: 0.1646\n",
      "Loss: 0.1620\n",
      "Loss: 0.1567\n",
      "Loss: 0.1552\n",
      "Loss: 0.1626\n",
      "Loss: 0.1609\n",
      "Loss: 0.1600\n",
      "Loss: 0.1560\n",
      "Loss: 0.1684\n",
      "Loss: 0.1438\n",
      "Loss: 0.1692\n",
      "Loss: 0.1530\n",
      "Loss: 0.1590\n",
      "Loss: 0.1595\n",
      "Loss: 0.1601\n",
      "Loss: 0.1594\n",
      "Loss: 0.1557\n",
      "Loss: 0.1569\n",
      "Loss: 0.1527\n",
      "Loss: 0.1535\n",
      "Loss: 0.1606\n",
      "Loss: 0.1566\n",
      "Loss: 0.1553\n",
      "Loss: 0.1540\n",
      "Loss: 0.1518\n",
      "Loss: 0.1560\n",
      "Loss: 0.1474\n",
      "Loss: 0.1501\n",
      "Loss: 0.1531\n",
      "Loss: 0.1585\n",
      "Loss: 0.1579\n",
      "Loss: 0.1637\n",
      "Loss: 0.1581\n",
      "Loss: 0.1497\n",
      "Loss: 0.1578\n",
      "Loss: 0.1643\n",
      "Loss: 0.1439\n",
      "Loss: 0.1575\n",
      "Loss: 0.1559\n",
      "Loss: 0.1516\n",
      "Loss: 0.1493\n",
      "Loss: 0.1512\n",
      "Loss: 0.1452\n",
      "Loss: 0.1547\n",
      "Loss: 0.1579\n",
      "Loss: 0.1586\n",
      "Loss: 0.1575\n",
      "Loss: 0.1540\n",
      "Loss: 0.1530\n",
      "Loss: 0.1641\n",
      "Loss: 0.1534\n",
      "Loss: 0.1578\n",
      "Loss: 0.1648\n",
      "Loss: 0.1490\n",
      "Loss: 0.1561\n",
      "Loss: 0.1571\n",
      "Loss: 0.1515\n",
      "Loss: 0.1539\n",
      "Loss: 0.1699\n",
      "Loss: 0.1580\n",
      "Loss: 0.1659\n",
      "Loss: 0.1676\n",
      "Loss: 0.1515\n",
      "Loss: 0.1618\n",
      "Loss: 0.1552\n",
      "Loss: 0.1521\n",
      "Loss: 0.1422\n",
      "Loss: 0.1530\n",
      "Loss: 0.1640\n",
      "Loss: 0.1595\n",
      "Loss: 0.1527\n",
      "Loss: 0.1555\n",
      "Loss: 0.1543\n",
      "Loss: 0.1475\n",
      "Loss: 0.1458\n",
      "Loss: 0.1536\n",
      "Loss: 0.1545\n",
      "Loss: 0.1519\n",
      "Loss: 0.1527\n",
      "Loss: 0.1523\n",
      "Loss: 0.1475\n",
      "Loss: 0.1606\n",
      "Loss: 0.1612\n",
      "Loss: 0.1528\n",
      "Loss: 0.1566\n",
      "Loss: 0.1501\n",
      "Loss: 0.1539\n",
      "Loss: 0.1522\n",
      "Loss: 0.1460\n",
      "Loss: 0.1596\n",
      "Loss: 0.1506\n",
      "Loss: 0.1578\n",
      "Loss: 0.1555\n",
      "Loss: 0.1510\n",
      "Loss: 0.1555\n",
      "Loss: 0.1516\n",
      "Loss: 0.1543\n",
      "Loss: 0.1529\n",
      "Loss: 0.1583\n",
      "Loss: 0.1496\n",
      "Loss: 0.1548\n",
      "Loss: 0.1452\n",
      "Loss: 0.1555\n",
      "Loss: 0.1620\n",
      "Loss: 0.1615\n",
      "Loss: 0.1528\n",
      "Loss: 0.1571\n",
      "Loss: 0.1580\n",
      "Loss: 0.1487\n",
      "Loss: 0.1563\n",
      "Loss: 0.1598\n",
      "Loss: 0.1547\n",
      "Loss: 0.1491\n",
      "Loss: 0.1487\n",
      "Loss: 0.1468\n",
      "Loss: 0.1586\n",
      "Loss: 0.1551\n",
      "Loss: 0.1621\n",
      "Loss: 0.1621\n",
      "Loss: 0.1557\n",
      "Loss: 0.1566\n",
      "Loss: 0.1550\n",
      "Loss: 0.1564\n",
      "Loss: 0.1486\n",
      "Loss: 0.1532\n",
      "Loss: 0.1810\n",
      "Loss: 0.1588\n",
      "Loss: 0.1643\n",
      "Loss: 0.1442\n",
      "Loss: 0.1514\n",
      "Loss: 0.1630\n",
      "Loss: 0.1636\n",
      "Loss: 0.1457\n",
      "Loss: 0.1617\n",
      "Loss: 0.1500\n",
      "Loss: 0.1599\n",
      "Loss: 0.1490\n",
      "Loss: 0.1545\n",
      "Loss: 0.1700\n",
      "Loss: 0.1495\n",
      "Loss: 0.1500\n",
      "Loss: 0.1606\n",
      "Loss: 0.1498\n",
      "Loss: 0.1465\n",
      "Loss: 0.1551\n",
      "Loss: 0.1538\n",
      "Loss: 0.1544\n",
      "Loss: 0.1558\n",
      "Loss: 0.1507\n",
      "Loss: 0.1653\n",
      "Loss: 0.1472\n",
      "Loss: 0.1478\n",
      "Loss: 0.1453\n",
      "Loss: 0.1525\n",
      "Loss: 0.1534\n",
      "Loss: 0.1479\n",
      "Loss: 0.1596\n",
      "Loss: 0.1483\n",
      "Loss: 0.1573\n",
      "Loss: 0.1460\n",
      "Loss: 0.1476\n",
      "Loss: 0.1531\n",
      "Loss: 0.1555\n",
      "Loss: 0.1521\n",
      "Loss: 0.1562\n",
      "Loss: 0.1522\n",
      "Loss: 0.1577\n",
      "Loss: 0.1590\n",
      "Loss: 0.1583\n",
      "Loss: 0.1553\n",
      "Loss: 0.1535\n",
      "Loss: 0.1579\n",
      "Loss: 0.1492\n",
      "Loss: 0.1462\n",
      "Loss: 0.1553\n",
      "Loss: 0.1625\n",
      "Loss: 0.1567\n",
      "Loss: 0.1468\n",
      "Loss: 0.1457\n",
      "Loss: 0.1564\n",
      "Loss: 0.1599\n",
      "Loss: 0.1545\n",
      "Loss: 0.1587\n",
      "Loss: 0.1474\n",
      "Loss: 0.1605\n",
      "Loss: 0.1628\n",
      "Loss: 0.1468\n",
      "Loss: 0.1500\n",
      "Loss: 0.1549\n",
      "Loss: 0.1562\n",
      "Loss: 0.1438\n",
      "Loss: 0.1618\n",
      "Loss: 0.1555\n",
      "Loss: 0.1586\n",
      "Loss: 0.1609\n",
      "Loss: 0.1537\n",
      "Loss: 0.1513\n",
      "Loss: 0.1497\n",
      "Loss: 0.1642\n",
      "Loss: 0.1461\n",
      "Loss: 0.1448\n",
      "Loss: 0.1507\n",
      "Loss: 0.1469\n",
      "Loss: 0.1500\n",
      "Loss: 0.1586\n",
      "Loss: 0.1529\n",
      "Loss: 0.1469\n",
      "Loss: 0.1456\n",
      "Loss: 0.1541\n",
      "Loss: 0.1507\n",
      "Loss: 0.1543\n",
      "Loss: 0.1519\n",
      "Loss: 0.1465\n",
      "Loss: 0.1481\n",
      "Loss: 0.1525\n",
      "Loss: 0.1686\n",
      "Loss: 0.1580\n",
      "Loss: 0.1462\n",
      "Loss: 0.1551\n",
      "Loss: 0.1548\n",
      "Loss: 0.1510\n",
      "Loss: 0.1467\n",
      "Loss: 0.1596\n",
      "Loss: 0.1495\n",
      "Loss: 0.1567\n",
      "Loss: 0.1509\n",
      "Loss: 0.1494\n",
      "Loss: 0.1521\n",
      "Loss: 0.1507\n",
      "Loss: 0.1523\n",
      "Loss: 0.1529\n",
      "Loss: 0.1434\n",
      "Epoch 3 Average Training Loss: 0.1573\n",
      "----------\n",
      "Total Training Time: 147.90 minutes\n"
     ]
    }
   ],
   "source": [
    "# Iterate the epochs to tain the model, need to know total time elapsed\n",
    "\n",
    "total_training_time = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  # Set the model to train mode\n",
    "  model.train()\n",
    "  \n",
    "  # Initialize the total_loss for the epoch\n",
    "  total_epoch_loss = 0\n",
    "\n",
    "  # Iterate throug the train_data_loder batches to train the model, i.e. update the weigts and parameters\n",
    "  for batch in train_data_loader:\n",
    "    # Move the batch to the computation device\n",
    "    input_ids = batch.to(device)\n",
    "    # print(input_ids)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "    # print(outputs)\n",
    "\n",
    "    # Retrieve the loss from the forward pass result\n",
    "    loss = outputs.loss\n",
    "    # print(loss)\n",
    "\n",
    "    # Backward pass through the model\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clear the gradients before the next batch back propagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    total_epoch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss for the epoch\n",
    "  avg_epoch_loss = total_epoch_loss / len(train_data_loader)\n",
    "  print(f'Epoch {epoch + 1} Average Training Loss: {avg_epoch_loss:.4f}')\n",
    "  print('-' * 10)\n",
    "\n",
    "total_training_time += (time.time() - start_time)\n",
    "\n",
    "print(f'Total Training Time: {total_training_time/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "90",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 90 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[402], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Disable the gradient calculation during validation, since it is not used\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;66;03m# Iterate throug the val_data_loder batches to train the model, i.e. update the weigts and parameters\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_data_loader:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Move the batch to the computation device\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[390], line 11\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 11\u001b[0m   input_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_message_clean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m   target_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCCP Response\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m   concat_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_seq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Concatenate the input and the target message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\e1079458\\work\\learn-ml\\learn_transformers\\ccp-simulator-ai\\.ccp_simulator_venv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 90"
     ]
    }
   ],
   "source": [
    "# Calculate the average loss on the validation dataset to evaluate how the model is really performing, and whether there is overfitting during training\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the total_loss for the validation\n",
    "total_val_loss = 0\n",
    "\n",
    "# Calculate the elapsed validation time\n",
    "start_time = time.time()\n",
    "\n",
    "# Disable the gradient calculation during validation, since it is not used\n",
    "with torch.no_grad():\n",
    "  # Iterate throug the val_data_loder batches to train the model, i.e. update the weigts and parameters\n",
    "  for batch in val_data_loader:\n",
    "    # Move the batch to the computation device\n",
    "    input_ids = batch.to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "    # print(outputs)\n",
    "\n",
    "    # Retrieve the loss from the forward pass result\n",
    "    loss = outputs.loss\n",
    "    # print(loss)\n",
    "\n",
    "    # Update the total loss\n",
    "    total_val_loss += loss.item()\n",
    "  \n",
    "  # Calculate the average loss for validation data\n",
    "  avg_val_loss = total_val_loss / len(val_data_loader)\n",
    "  print(f'Average Validation Loss: {avg_val_loss:.4f}')\n",
    "  print('-' * 10)\n",
    "\n",
    "val_time = time.time() - start_time\n",
    "\n",
    "print(f'Validation Time: {val_time/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function using the trained model\n",
    "\n",
    "def predict(input_message, model):\n",
    "  # Calculate the prediction time of the model\n",
    "  start_time = time.time()\n",
    "  # Tokenize the input message\n",
    "  tokenized_input = tokenizer.encode(\n",
    "    input_message, \n",
    "    return_tensors='pt', \n",
    "    max_length=MAX_LENGTH, \n",
    "    truncation=True, \n",
    "    padding='max_length')\n",
    "\n",
    "  # Move the tokenized input to the computation device\n",
    "  tokenized_input = tokenized_input.to(device)\n",
    "\n",
    "  # Forward pass through the model\n",
    "  outputs = model.generate(\n",
    "    input_ids=tokenized_input, \n",
    "    max_length=MAX_LENGTH,\n",
    "    num_return_sequences=1, \n",
    "    pad_token_id = tokenizer.eos_token_id)\n",
    "  \n",
    "  # Retrieve the predicted output\n",
    "  predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  # Calculate the prediction time of the model\n",
    "  prediction_time = time.time() - start_time\n",
    "  print(f'Prediction Time: {prediction_time:.2f} seconds')\n",
    "\n",
    "  return predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up the 3rd observation from the validation dataset to compare later with the predicted one\n",
    "actual_output = val_data.iloc[2]['CCP Response'] # This syntax is more efficient than val_data['CCP Response'][2] especially for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up the third input message from the validation dataset\n",
    "input_message = val_data.iloc[2]['input_message_clean']\n",
    "\n",
    "# Predict the CCP response using the trained model\n",
    "\n",
    "predicted_output = predict(input_message, model=model)\n",
    "\n",
    "print(f'Output successfuly predicted: {predicted_output == actual_output}')\n",
    "print(f'Predicted CCP Response: {predicted_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of the model\n",
    "\n",
    "def calculate_accuracy(model, val_data):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Iterate through the validation data, and compare the output with actual using the predict function\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "\n",
    "  for i in range(len(val_data)):\n",
    "    input_message = val_data.iloc[i]['input_message_clean']\n",
    "    actual_output = val_data.iloc[i]['CCP Response']\n",
    "\n",
    "    predicted_output = predict(input_message, model=model)\n",
    "\n",
    "    if predicted_output == actual_output:\n",
    "      total_correct += 1\n",
    "    \n",
    "    total_samples += 1\n",
    "\n",
    "  # Calculate the accuracy and print it\n",
    "  accuracy = total_correct / total_samples\n",
    "  print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "calculate_accuracy(model=model, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify the wrong predicted messages to print them\n",
    "\n",
    "def identify_wrong_predictions(model, val_data):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Iterate through the validation data, and compare the output with actual using the predict function\n",
    "  for i in range(len(val_data)):\n",
    "    input_message = val_data.iloc[i]['input_message_clean']\n",
    "    actual_output = val_data.iloc[i]['CCP Response']\n",
    "\n",
    "    predicted_output = predict(input_message, model=model)\n",
    "\n",
    "    if predicted_output != actual_output:\n",
    "      # Print index of the failing prediction\n",
    "      print(f'Message {i}')\n",
    "      # Print the input message, actual CCP response and predicted CCP response\n",
    "      print(f'Actual CCP Response: {actual_output}')\n",
    "      print(f'Predicted CCP Response: {predicted_output}')\n",
    "      print(f'Input Message: {input_message}')\n",
    "      print('-' * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input sample and predict the output using the templates at the beginning of the notebook\n",
    "generated_sample = generate_full_service_wf()\n",
    "\n",
    "# Concatenate the first and the second messages from the generated_sample tuple\n",
    "input_seq = generated_sample[0] + ' ' + generated_sample[1]\n",
    "input_seq = clean_text(input_seq)\n",
    "\n",
    "print(f'Input Message: {input_seq}')\n",
    "\n",
    "\n",
    "\n",
    "predicted_output = predict(input_seq, model=model)\n",
    "\n",
    "print(f'Predicted CCP Response: {predicted_output}')\n",
    "print(f'Actual CCP Reponse: {generated_sample[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a specified Kaggle directory\n",
    "\n",
    "# model.save_pretrained(save_directory=\"/kaggle/working/full_service_model\")\n",
    "model.save_pretrained(save_directory=\"./models/full_service_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the saved model using GPTLMHeadModel\n",
    "model_reloaded = GPT2LMHeadModel.from_pretrained(\"./models/full_service_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the pre-trained model to HuggingFace Hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # Prompt the user to login\n",
    "\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi()\n",
    "\n",
    "# api.create_repo(repo_id=\"full_service_model\", private=True)\n",
    "\n",
    "model_reloaded.push_to_hub(\"soulou2000/full_service_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model from HF Hub\n",
    "\n",
    "model_reloaded_from_hf = GPT2LMHeadModel.from_pretrained(\"soulou2000/full_service_model\", token='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')\n",
    "model_reloaded_from_hf.to(device)\n",
    "# Generate a new CCP response using the pre-trained model, and make sure we get the same output as earlier\n",
    "predicted_output = predict(input_seq, model=model_reloaded_from_hf)\n",
    "\n",
    "print(f'Predicted CCP Response: {predicted_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ccp_simulator_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
