{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only needed when working from local environment, due to local SSL security when loading the HuggingFace model and token below. Remove this when working from Colab or Kaggle for example\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9960914254188538}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "result = classifier('Nothing has been done !')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"When planning for a project, we need first to identify the best fit for you. If a project is not an excellent option, then you have to build it so that we can make it look nicer to users. What's more, you can save\"}, {'generated_text': \"When planning for a project, we need first to identify the potential value of the project that we are making. For instance, the company's mission to create a high quality home-built for the community is to create a new low profile home-built\"}, {'generated_text': 'When planning for a project, we need first to identify the best fit and budget with the company that is running the work.â€º\\n\\n\\n\\n\\nFor instance, most of the work is going on at the state level, so the first'}]\n"
     ]
    }
   ],
   "source": [
    "# Text generation model\n",
    "\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "result = generator(\n",
    "  'When planning for a project, we need first to identify the', \n",
    "  max_length=50, do_sample=True,\n",
    "  truncation=True,\n",
    "  top_k=50, \n",
    "  top_p=0.95, \n",
    "  num_return_sequences=3)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about geo-politics', 'labels': ['politics', 'education', 'business'], 'scores': [0.9454228281974792, 0.029302459210157394, 0.02527468465268612]}\n"
     ]
    }
   ],
   "source": [
    "# Perform zero-shot classification\n",
    "\n",
    "# classifier = pipeline('zero-shot-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "\n",
    "result = classifier(\n",
    "  \"This is a course about geo-politics\",\n",
    "  candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    "  # multi_label=True\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer:DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "result:{'input_ids': [101, 1045, 2514, 3407, 2055, 2023, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokens: ['i', 'feel', 'happy', 'about', 'this']\n",
      "ids: [1045, 2514, 3407, 2055, 2023]\n",
      "tokenized_sequence: [101, 1045, 2514, 3407, 2055, 2023, 102]\n",
      "decoded_string: i feel happy about this\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "input_seq = \"I feel happy about this\"\n",
    "\n",
    "print(f'tokenizer:{tokenizer}')\n",
    "result = tokenizer(input_seq)\n",
    "print(f'result:{result}')\n",
    "\n",
    "tokens = tokenizer.tokenize(input_seq) # Split seq into words; still in string format\n",
    "print(f'Tokens: {tokens}')\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # Provide the numeric form of the string tokens\n",
    "print(f'ids: {ids}')\n",
    "\n",
    "tokenized_sequence = tokenizer.encode(input_seq) # Add BoS (Beginning of Sentence) and EoS (i.e. End of Sentence) token ids to the tokenized sequence; all in numeric form\n",
    "# print tokenized_sequence\n",
    "print(f'tokenized_sequence: {tokenized_sequence}')\n",
    "\n",
    "decoded_string = tokenizer.decode(ids) # Retrieve the text form\n",
    "print(f'decoded_string: {decoded_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(input_seq, padding=True, truncation=True, max_length=10, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2514, 3407, 2055, 2023,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "logits: tensor([[-4.3432,  4.6859]])\n",
      "probs: tensor([[1.1986e-04, 9.9988e-01]])\n",
      "probs[0]: tensor([1.1986e-04, 9.9988e-01])\n",
      "probs[0][0]: 0.0001198602476506494\n",
      "probs[0][1]: 0.9998800754547119\n",
      "tensor([[1.1986e-04, 9.9988e-01]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Perform the inference with pytorch\n",
    "with torch.no_grad():\n",
    "  # input_ids = torch.tensor(tokenized_sequence).unsqueeze(0)\n",
    "  # input_ids = torch.tensor(tokenized_sequence).unsqueeze(0)\n",
    "  print(batch)\n",
    "  outputs = model(**batch) # Unpack the dictionary batch as separate arguments to the model\n",
    "  logits = outputs[0]\n",
    "  probs = F.softmax(logits, dim=1)\n",
    "  print(f'logits: {logits}')\n",
    "  print(f'probs: {probs}')\n",
    "  print(f'probs[0]: {probs[0]}')\n",
    "  print(f'probs[0][0]: {probs[0][0]}')\n",
    "  print(f'probs[0][1]: {probs[0][1]}')\n",
    "  predictions = F.softmax(logits, dim=1)\n",
    "  print(predictions)\n",
    "  labels = torch.argmax(predictions, dim=1)\n",
    "  print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ccp_simulator_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
